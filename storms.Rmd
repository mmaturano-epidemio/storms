---
title: "On the human and economical impact of severe weather events"
output: html_document
author: Manuel Maturano
date: "2026-01-14"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
dataset <- readRDS("tidy_storms.rds")
knitr::opts_chunk$set(echo = TRUE)
```

# On the human and economical impact of severe weather events

Title: Your document should have a title that briefly summarizes your data analysis

## Synopsis

Synopsis: Immediately after the title, there should be a synopsis which describes and summarizes your analysis in at most 10 complete sentences.

The present analysis employs the National Oceanic & Atmospheric Administration (NOAA) and aims to quantify the impact in terms of human health and economic loses of extreme weather events (i. e.: tornadoes, extreme cold), in order to determine which ones are the most harmful.

## Data Processing

The data processing section is divided into two parts. In the first part, *Data Cleaning*, we describe the characteristics of the original raw data (the NOAA Storm Database), the justification for the transformations required before it could be suitable for statistical analysis, and how these were implemented.

In the second part, *Data Analysis*, we describe and justify the analytic plan and its implementation. The rationale for performing this here, rather than in the Results section, is to ensure the latter focuses primarily on the exposition of findings, free from long fragments of code, and thus remains more transparent to the reader.

### Data cleaning
During the initial processing steps, it becomes evident that the dataset is not suitable for use in its raw state. Dates are in an irregular format, preventing a clear identification of the year for each event —a variable crucial to this analysis. Event names are also problematic; they do not strictly reflect the 48 categories recognized by NOAA, but rather include hundreds of variations, irrelevant entries, and misplaced data. Furthermore, many columns contain missing or redundant information that must be removed for computational efficiency.

Additionally, certain variables necessary for a comprehensive geographical analysis (such as Region) are missing and must be constructed. Monetary losses are encoded using character suffixes and need to be converted into a numeric format for calculation. Finally, an imputation error regarding the losses of a specific disaster was identified and requires correction. The implementation of these steps and the rationale behind the decision-making process are detailed below.


We start by loading the packages and raw data we require:

```{r dataset, include=TRUE, echo=FALSE}

# ---- Get files and libraries ----
# First we get the package manager and libraries we'll need
if(!require(pacman)){
  install.packages("pacman")
}
pacman::p_load(data.table, 
               ggplot2, 
               lubridate,
               flextable,
               skimr,
               stringr)

# Now we download the data if needed, and read the file
if(!file.exists("FStormData.csv")){
  download.file(url = "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2",
                destfile = file.path(getwd(), "FStormData.csv"))
}

storms <- fread("FStormData.csv", 
                na.strings = "", 
                showProgress = TRUE, 
                verbose = TRUE) |> janitor::clean_names()

# clean_names here is included so that the dataset has tidy column names
```

Now we explore the structure of the data and start with the necessary transformations:

```{r str, include=TRUE, echo=FALSE}
str(storms)

# Date columns are in character format
# The time, range, location, and size related columns are not relevant to our problem
# Property and crop damage columns have coded values. 
# They need to be exponentiated using propdmgexp and cropdmgexp.

# We'll start by keeping only the columns we'll require:

tidy_storms <- copy(storms)

tidy_storms <- tidy_storms[, .(bgn_date, 
                               end_date, 
                               state = as.factor(state),
                               state_2 = as.factor(state_2), 
                               county = as.factor(county),
                               countyname, 
                               evtype, 
                               fatalities,
                               injuries, 
                               propdmg, 
                               propdmgexp, 
                               cropdmg, 
                               cropdmgexp,
                               remarks, 
                               refnum)]


# Now, the date columns are in character format, and times are uninformative:
dates <- names(tidy_storms)[names(tidy_storms) %like% "date"] 
tidy_storms[!is.na(end_date), head(.SD, 5), .SDcols = dates]
tidy_storms[!is.na(end_date), tail(.SD, 5), .SDcols = dates]

# Therefore, we format these dates:
tidy_storms[, (dates) := lapply(.SD, sub, pattern = " .*$", replacement = ""), .SDcols = dates]
tidy_storms[, (dates) := lapply(.SD, mdy), .SDcols = dates]

# And we create an "year" column
tidy_storms[, year := year(bgn_date)]
```
Now we'll see if there's too much variability on reporting among the years:

```{r}
tidy_storms[, .(`Reported events` = uniqueN(evtype)), year]
tidy_storms[year%in%c(seq(1950, 1990, 5)), 
            .(`Reported events` = unique(evtype)), year]

```
We can see that most years in the dataset only consider three events: Hail, tornado, and Tstm Wind. Including all these years could heavily bias the analysis in favor of these events, simply because there would be more data available about them. Therefore, we'll then exclude these years.
```{r}
tidy_storms <- tidy_storms[year > 1992]
```

Now, we're going to see how many NA (non-available data) rows we have in each column, which could be an issue:
```{r}
tidy_storms[, colMeans(is.na(.SD)) |> round(2)] |> sort(decreasing = TRUE)

```

We see the end dates have NAs, which shouldn't be a problem since we have the beginning date for every event. Crop and property exponents are often missing, so we will assume those rows reflect the actual value on USD and don't need exponentiation.

We'll explore the exponents now and apply them:

```{r}
tidy_storms[, .(propdmgexp = unique(propdmgexp))] |> c()
tidy_storms[, .(cropdmgexp = unique(cropdmgexp))] |> c()

# We see the exponents aren't computed in an uniform fashion.
# We'll assume non-specific values (i.e.: "?", "-") and NAs equal to 
# no exponentiation required (propdmg/cropdmg we'll be taken as-is).
# We will assume H = 100; K = 1,000; M = 1,000,000; B = 1,000,000,000.
# We'll treat lower case as caps.

map_exp <- function(x) {
  x <- toupper(as.character(x))
  fcase(
    x %like% "H", 1e2,
    x %like% "K", 1e3,
    x %like% "M", 1e6,
    x %like% "B", 1e9,
    is.na(x) | x %chin% c("", "-", "?", "+"), 1, 
    x %chin% as.character(0:9), 10^as.numeric(x),  
    default = 1
  )
}

tidy_storms[, prop_total := propdmg * map_exp(propdmgexp)]
tidy_storms[, crop_total := cropdmg * map_exp(cropdmgexp)]
tidy_storms[, economic_loss := prop_total + crop_total]
```

Now, we'll explore the events. 
```{r}
# Acording to NOAA, the types of events are:
# Astronomical High Tide, Astronomical Low Tide, Avalanche, Blizzard, 
# Coastal Flood, Cold/Wind Chill, Debris Flow, Dense Fog, Dense Smoke, Drought,
# Dust Devil, Dust Storm, Excessive Heat, 
# Extreme Cold/Wind Chill, Flash Flood, Flood, Freezing Fog,
# Frost/Freeze, Funnel Cloud, Hail, Heat, Heavy Rain, Heavy Snow,
# High Surf, High Wind, Hurricane/Typhoon, Ice Storm, 
# Lakeshore Flood, Lake-Effect Snow, Lightning, Marine Hail,
# Marine High Wind, Marine Strong Wind, Marine Thunderstorm Wind,
# Rip Current, Seiche, Sleet, Storm Tide, Strong Wind, Thunderstorm Wind,
# Tornado, Tropical Depression, Tropical Storm, Tsunami, 
# Volcanic Ash, Waterspout, Wildfire, Winter Storm, Winter Weather

# Unify evtype format:
tidy_storms[, evtype := stringr::str_to_title(evtype)]

# We see the first 10 evtypes
tidy_storms[, .(evtype = unique(evtype))][, head(evtype, 10)] 

# We count how many evtypes we have
tidy_storms[, uniqueN(evtype)] 
```

 These are way too many events. We have to carefully consider how to group them.
 First, we will eliminate the evtypes named "summary", which are uninformative, and then we will group the remaining events in cathegories as close as the ones recognized by NOAA as we can.
```{r}

# Eliminate "summary" rows
tidy_storms <- tidy_storms[!evtype %like% "(?i)Summary"]

# Create "clean_event" 
tidy_storms[, clean_event := fcase(
  evtype %like% "(?i)Flash Flood", "Flash Flood",  
  evtype %like% "(?i)Ligh?tning|Ligntning", "Lightning", 
  evtype %like% "(?i) Devil|Devel", "Dust Devil", 
  evtype %like% "(?i)Dust Storm", "Dust Storm", 
  evtype %like% "(?i)Lakeshore Flood", "Lakeshore Flood",  
  evtype %like% "(?i)Funnel Cloud", "Funnel Cloud", 
  evtype %like% "(?i)Coastal Flood|Cstl Flood|Tidal Flood", "Coastal Flood", 
  evtype %like% "(?i)Lake Effect Snow", "Lake-Effect Snow", 
  evtype %like% "(?i)Marine (Thunderstorm|Tstm)", "Marine Thunderstorm Wind", 
  evtype %like% "(?i)Winter Storm", "Winter Storm", 
  evtype %like% "(?i)Winter Weather", "Winter Weather", 
  evtype %like% "(?i)Tornado|Torn?da?o|Gustnado|Landspout", "Tornado",  
  evtype %like% "(?i)Ice Storm", "Ice Storm", 
  evtype %like% "(?i)Thunderstorm|Tstm|Storm Force|Burst|Thund?e?e?r?sto?r?m", "Thunderstorm Wind",
  evtype %like% "(?i)Flood|Fld|Stream", "Flood",  
  evtype %like% "(?i)Surf|High Tide|High Water|Swells|Waves", "High Surf",  
  evtype %like% "(?i)Hurricane|Typhoon", "Hurricane/Typhoon", 
  evtype %like% "(?i)Marine Hail", "Marine Hail",  
  evtype %like% "(?i)Hail", "Hail", 
  evtype %like% "(?i)Excessive Heat", "Excessive Heat", 
  evtype %like% "(?i)Heat|Warm|Hot", "Heat",  
  evtype %like% "(?i)Extreme Cold|Extreme Wind Chill", "Extreme Cold/Wind Chill",  
  evtype %like% "(?i)Cold|Wind Chill|Hypothermia", "Cold/Wind Chill", 
  evtype %like% "(?i)Tropical Storm", "Tropical Storm", 
  evtype %like% "(?i)Tropical Depression", "Tropical Depression", 
  evtype %like% "(?i)Tsunami", "Tsunami", 
  evtype %like% "(?i)Drought", "Drought", 
  evtype %like% "(?i)Smoke", "Dense Smoke", 
  evtype %like% "(?i)Avalanche", "Avalanche", 
  evtype %like% "(?i)Current", "Rip Current",
  evtype %like% "(?i)Astronomical High", "Astronomical High Tide",
  evtype %like% "(?i)Astronomical Low", "Astronomical Low Tide",
  evtype %like% "(?i)Seiche", "Seiche",
  evtype %like% "(?i)Sleet", "Sleet",
  evtype %like% "(?i)Sleet", "Sleet",
  evtype %like% "(?i)Tidem|Tide", "Storm Tide",
  evtype %like% "(?i)Volcanic Ash", "Volcanic Ash", 
  evtype %like% "(?i)High Wind", "High Wind", 
  evtype %like% "(?i)Marine Strong Wind", "Marine Strong Wind", 
  evtype %like% "(?i)Marine High Wind", "Marine High Wind", 
  evtype %like% "(?i)Strong Wind|Gusty", "Strong Wind",
  evtype %like% "(?i)Snow", "Heavy Snow", 
  evtype %like% "(?i)Rain|Shower|Precip", "Heavy Rain", 
  evtype %like% "(?i)Wildfire|Fire|Forest Fire", "Wildfire", 
  evtype %like% "(?i)Freezing Fog", "Freezing Fog", 
  evtype %like% "(?i)Fog", "Dense Fog", 
  evtype %like% "(?i)Waterspout", "Waterspout", 
  evtype %like% "(?i)Blizzard", "Blizzard", 
  evtype %like% "(?i)Frost|Freeze", "Frost/Freeze", 
  evtype %like% "(?i)Landslide|Mud ?slide|Debris|Rock Slide", "Debris Flow",
  evtype %like% "(?i)Storm Surge|Coastal ?storm", "Storm Surge/Tide",
  evtype %like% "(?i)Extreme Windchill|Low Temp|Hypothermia|Exposure", "Extreme Cold/Wind Chill",
  evtype %like% "(?i)Glaze|Ice (On Road|Roads)|Black Ice|Freezing (Drizzle|Spray)|Wintry Mix", "Winter Weather",
  evtype %like% "(?i)^Winds?$", "Strong Wind",
  evtype %like% "(?i)Thundertorm|Tunderstorm", "Thunderstorm Wind",
  evtype %like% "(?i)Rough Seas|High Seas|Heavy Seas|Marine (Mishap|Accident)|Drowning", "High Surf",
  evtype %like% "(?i)Avalance", "Avalanche",
  evtype %like% "(?i)Whirlwind", "Tornado",
  evtype %like% "(?i)Rapidly Rising Water", "Flash Flood",
  evtype %like% "(?i)Other|Summary|\\?|None", "Other",
    default = evtype
)]

tidy_storms[, uniqueN(clean_event)]

```
We have significantly reduced the types of events, and keept only a `r round(tidy_storms[, uniqueN(clean_event)] / tidy_storms[, uniqueN(evtype)] *100, 2)`% of the original ones.

As said before, we may also need to analyse events geographically, so we will create a region variable:

```{r}
northeast <- c("CT", "ME", "MA", "NH", "RI", "VT", "NJ", "NY", "PA")
midwest   <- c("IL", "IN", "MI", "OH", "WI", "IA", "KS", "MN", "MO", "NE", "ND", "SD")
south     <- c("DE", "FL", "GA", "MD", "NC", "SC", "VA", "DC", "WV", "AL", "KY", "MS", "TN", "AR", "LA", "OK", "TX")
west      <- c("AZ", "CO", "ID", "MT", "NV", "NM", "UT", "WY", "AK", "CA", "HI", "OR", "WA")

tidy_storms[, region := fcase(
  state_2 %in% northeast, "Northeast",
  state_2 %in% midwest,   "Midwest",
  state_2 %in% south,     "South",
  state_2 %in% west,      "West",
  state_2 %in% c("PR", "GU", "AS", "VI", "MH", "AM"), "Territories",
  default = "Marine/Other"
)]

```

Finally, on the data exploration a significant error was detected:
```{r}

tidy_storms[countyname == "NAPA" & year == 2006, .(evtype, propdmg, propdmgexp, remarks)]
```

 We can see that the damage recorded on the "remarks" was of some million USD, but the computed propdmgexp is B. We need to fix this:
 
```{r}



# Correcting the 2006 Napa Valley outlier
tidy_storms[countyname == "NAPA" & year == 2006 & propdmgexp == "B", 
            `:=`(prop_total = prop_total / 1000, 
                 economic_loss = (prop_total / 1000) + crop_total)]

```

### Data analysis
 
 In this section, we will produce the objects that will be shown on the results section.
```{r}

# ---- Human loses ----
total_fatalities <- tidy_storms[, sum(fatalities)]
total_injuries <- tidy_storms[, sum(injuries)]
tidy_storms[, .(fatalities = sum(fatalities),
                 fatalities_pct = round(sum(fatalities) / total_fatalities * 100, 2),
                 injuries = sum(injuries),
                 injuries_pct = round(sum(injuries) / total_injuries * 100, 2)), 
             evtype][order(-fatalities)][
               !(injuries == 0 & fatalities == 0)][1:20]
tidy_storms[countyname == "NAPA" & year == 2006, .(evtype, propdmg, propdmgexp, remarks)]

# ---- Economic loses ----
total_loss <- tidy_storms[, sum(economic_loss)]
tidy_storms[, .(economic_loss = sum(economic_loss, na.rm = TRUE), 
                 economic_loss_pct = round(sum(economic_loss, na.rm = TRUE) / total_loss *100, 2),
                 total_events = .N), 
             by = clean_event][order(-economic_loss)][economic_loss > 0]

tidy_storms[, unique(state)]
tidy_storms[, unique(state_2)]

# ---- Total loses ----
# Definir valores de monetización
vsl_value <- 10e6    # 10 millones USD
injury_value <- 5e5  # 500 mil USD

# Calcular Carga de Salud y Carga Total
tidy_storms[, health_loss := (fatalities * vsl_value) + (injuries * injury_value)]
tidy_storms[, total_burden := economic_loss + health_loss]

# Top 10 para el Manager (Tabla resumen)
top_10_events <- tidy_storms[, .(
  total_burden_B = sum(total_burden) / 1e9,
  economic_loss_B = sum(economic_loss) / 1e9,
  health_loss_B = sum(health_loss) / 1e9,
  fatalities = sum(fatalities),
  total_events = .N
), by = clean_event][order(-total_burden_B)][1:10]

# Ver tabla
flextable::flextable(top_10_events) |> 
  flextable::colformat_double(digits = 2)

# ---- By region ----
northeast <- c("CT", "ME", "MA", "NH", "RI", "VT", "NJ", "NY", "PA")
midwest   <- c("IL", "IN", "MI", "OH", "WI", "IA", "KS", "MN", "MO", "NE", "ND", "SD")
south     <- c("DE", "FL", "GA", "MD", "NC", "SC", "VA", "DC", "WV", "AL", "KY", "MS", "TN", "AR", "LA", "OK", "TX")
west      <- c("AZ", "CO", "ID", "MT", "NV", "NM", "UT", "WY", "AK", "CA", "HI", "OR", "WA")

tidy_storms[, region := fcase(
  state_2 %in% northeast, "Northeast",
  state_2 %in% midwest,   "Midwest",
  state_2 %in% south,     "South",
  state_2 %in% west,      "West",
  state_2 %in% c("PR", "GU", "AS", "VI", "MH", "AM"), "Territories",
  default = "Marine/Other"
)]


# 1. El Top 15 por severidad (vector de caracteres)
top_15_names <- tidy_storms[, .(t = sum(total_burden)), by = clean_event][order(-t)][1:15, clean_event]

# 2. Todas las regiones (incluyendo Marine/Other para que no quede el hueco blanco)
# Nota: Usamos el nombre exacto que pusiste en el fcase original
regions_vec <- unique(tidy_storms$region) 

# 3. El orden de las facetas que querías
niveles_region <- c("Northeast", "Midwest", "South", "West", "Territories", "Marine/Other")

# A. Cuadrícula completa
complete_grid <- CJ(year = unique(tidy_storms$year),
                    clean_event = top_15_names,
                    region = regions_vec)

# B. Datos agregados (sin transformar factores todavía)
heatmap_regiones_actual <- tidy_storms[clean_event %in% top_15_names, 
                                        .(burden = sum(total_burden)), 
                                        by = .(year, clean_event, region)]

# C. Join limpio: Traemos los datos a la cuadrícula completa
heatmap_full <- heatmap_regiones_actual[complete_grid, on = .(year, clean_event, region)]

# D. Rellenar NAs con 0
heatmap_full[is.na(burden), burden := 0]
# Orden de regiones para las facetas
heatmap_full[, region := factor(region, levels = niveles_region)]

# Orden de eventos: el más severo (Flood) quedará al final del factor, o sea, ARRIBA
heatmap_full[, clean_event := factor(clean_event, levels = rev(top_15_names))]
ggplot(heatmap_full, aes(x = year, y = clean_event, fill = burden)) +
  geom_tile(color = "white", linewidth = 0.05) +
  scale_fill_viridis_c(
    trans = "log10", 
    labels = scales::label_dollar(scale = 1e-6, suffix = "M"),
    na.value = "#440154", 
    name = "Carga Total\n(Eco + Salud)"
  ) + 
  facet_wrap(~region, ncol = 2) +
  theme_minimal(base_size = 11) + 
  labs(
    title = "Jerarquía de Severidad por Región y Año",
    subtitle = "Eventos ordenados por impacto global acumulado (más severos arriba)",
    x = "Año", y = ""
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 9),
    strip.background = element_rect(fill = "gray95", color = NA),
    strip.text = element_text(face = "bold"),
    panel.grid = element_blank(),
    legend.position = "bottom",
    legend.key.width = unit(2, "cm")
  )
graphics.off()

```

## Results

There should be a section titled Results in which your results are
presented.

You may have other sections in your analysis, but Data Processing and
Results are required.

The analysis document must have at least one figure containing a plot.

Your analysis must have no more than three figures. Figures may have
multiple plots in them (i.e. panel plots), but there cannot be more than
three figures total.

You must show all your code for the work in your analysis document. This
may make the document a bit verbose, but that is okay. In general, you
should ensure that echo = TRUE echo = TRUEstart color red, start
verbatim, echo = TRUE, end verbatim, end color red for every code chunk
(this is the default setting in knitr).

Assignment The basic goal of this assignment is to explore the NOAA
Storm Database and answer some basic questions about severe weather
events. You must use the database to answer the questions below and show
the code for your entire analysis. Your analysis can consist of tables,
figures, or other summaries. You may use any R package you want to
support your analysis.

Questions Your data analysis must address the following questions:

Across the United States, which types of events (as indicated in the
EVTYPE EVTYPEstart color red, start verbatim, EVTYPE, end verbatim, end
color red variable) are most harmful with respect to population health?

Across the United States, which types of events have the greatest
economic consequences?

```         
Consider writing your report as if it were to be read by a government or municipal manager who might be responsible for preparing for severe weather events and will need to prioritize resources for different types of events. However, there is no need to make any specific recommendations in your report.
```

Requirements For this assignment you will need some specific tools

RStudio: You will need RStudio to publish your completed analysis
document to RPubs. You can also use RStudio to edit/write your analysis.

knitr: You will need the knitr package in order to compile your R
Markdown document and convert it to HTML

Document Layout Language: Your document should be written in English.

```{r cars}
summary(cars)
```

## Storms

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to
prevent printing of the R code that generated the plot.
